import openai
import logging
from trialcurator.llm_client import LlmClient

logger = logging.getLogger(__name__)


class OpenaiClient(LlmClient):
    """
    A client for interacting with OpenAI's language models.

    This class wraps the OpenAI Python client and provides access to the
    OpenAI GPT models for generating text completions based on provided prompts.
    It extends the `LlmClient` abstract base class.
    """
    MODEL = "gpt-4o"
    TOP_P = 1.0

    def __init__(self, temperature, top_p=TOP_P, model=MODEL):
        """
        Initialize the OpenaiClient class with specific model and tuning parameters.

        Parameters:
            temperature (float): Sampling temperature, controlling randomness in generated responses.
            top_p (float): Nucleus sampling value, controlling diversity in generated responses.
            model (str): The name of the OpenAI model to use (defaults to "gpt-4o").
        """
        self.wrapped_client = openai.Client()
        self.temperature = temperature
        self.top_p = top_p
        self.model = model

    def llm_ask(self, user_prompt: str, system_prompt: str = None) -> str:
        """
        Send a prompt to the OpenAI language model and return the generated response.

        The method logs the prompt and the response at the INFO level for debugging.

        Parameters:
            system_prompt (str) (optional): The text prompt that sets the context/role the llm is to take on. E.g. An expert curator to interpret the rules or an assistant to simplify the language of the eligibility criteria
            user_prompt (str): The actual instructions to the llm. E.g. Extract the molecular criteria from this condition.

        Returns:
            str: The text response generated by the language model.
        """

        messages = []
        if system_prompt is not None:
            messages.append({"role": "system", "content": system_prompt})
            for line in system_prompt.splitlines():
                logging.info(f'system prompt: {line}')

        messages.append({"role": "user", "content": user_prompt})
        for line in user_prompt.splitlines():
            logging.info(f'prompt: {line}')

        completion = (self.wrapped_client.chat.completions.create(
            model=self.model,
            temperature=self.temperature,
            top_p=self.top_p,
            messages=messages
        ))

        # log the response
        response = completion.choices[0].message.content
        for line in response.splitlines():
            logging.info(f'response: {line}')

        return response
